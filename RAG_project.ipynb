{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guy998877/projects/blob/main/RAG_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OvkPji9O-qX"
      },
      "source": [
        "# Tutorial: Creating Your First QA Pipeline with Retrieval-Augmentation\n",
        "\n",
        "- **Level**: Beginner\n",
        "- **Time to complete**: 10 minutes\n",
        "- **Components Used**: [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore), [`SentenceTransformersDocumentEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder), [`SentenceTransformersTextEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder), [`InMemoryEmbeddingRetriever`](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever), [`PromptBuilder`](https://docs.haystack.deepset.ai/docs/promptbuilder), [`OpenAIGenerator`](https://docs.haystack.deepset.ai/docs/openaigenerator)\n",
        "- **Prerequisites**: You must have an [OpenAI API Key](https://platform.openai.com/api-keys).\n",
        "- **Goal**: After completing this tutorial, you'll have learned the new prompt syntax and how to use PromptBuilder and OpenAIGenerator to build a generative question-answering pipeline with retrieval-augmentation.\n",
        "\n",
        "> This tutorial uses Haystack 2.0. To learn more, read the [Haystack 2.0 announcement](https://haystack.deepset.ai/blog/haystack-2-release) or visit the [Haystack 2.0 Documentation](https://docs.haystack.deepset.ai/docs/intro)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFqHcXYPO-qZ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows you how to create a generative question-answering pipeline using the retrieval-augmentation ([RAG](https://www.deepset.ai/blog/llms-retrieval-augmentation)) approach with Haystack 2.0. The process involves four main components: [SentenceTransformersTextEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder) for creating an embedding for the user query, [InMemoryBM25Retriever](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever) for fetching relevant documents, [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder) for creating a template prompt, and [OpenAIGenerator](https://docs.haystack.deepset.ai/docs/openaigenerator) for generating responses.\n",
        "\n",
        "For this tutorial, you'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents, but you can replace them with any text you want.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXjVlbPiO-qZ"
      },
      "source": [
        "## Preparing the Colab Environment\n",
        "\n",
        "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration)\n",
        "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/logging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kww5B_vXO-qZ"
      },
      "source": [
        "## Installing Haystack\n",
        "\n",
        "Install Haystack 2.0 and other required packages with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQbU8GUfO-qZ"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai\n",
        "pip install \"datasets>=2.6.1\"\n",
        "pip install \"sentence-transformers>=2.2.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl_jYERtO-qa"
      },
      "source": [
        "### Enabling Telemetry\n",
        "\n",
        "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/enabling-telemetry) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A76B4S49O-qa"
      },
      "outputs": [],
      "source": [
        "from haystack.telemetry import tutorial_running\n",
        "\n",
        "tutorial_running(27)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lvfew16O-qa"
      },
      "source": [
        "## Fetching and Indexing Documents\n",
        "\n",
        "You'll start creating your question answering system by downloading the data and indexing the data with its embeddings to a DocumentStore.\n",
        "\n",
        "In this tutorial, you will take a simple approach to writing documents and their embeddings into the DocumentStore. For a full indexing pipeline with preprocessing, cleaning and splitting, check out our tutorial on [Preprocessing Different File Types](https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline).\n",
        "\n",
        "\n",
        "### Initializing the DocumentStore\n",
        "\n",
        "Initialize a DocumentStore to index your documents. A DocumentStore stores the Documents that the question answering system uses to find answers to your questions. In this tutorial, you'll be using the `InMemoryDocumentStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbVN-s5LO-qa"
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8nuJdWO-qa"
      },
      "source": [
        "> `InMemoryDocumentStore` is the simplest DocumentStore to get started with. It requires no external dependencies and it's a good option for smaller projects and debugging. But it doesn't scale up so well to larger Document collections, so it's not a good choice for production systems. To learn more about the different types of external databases that Haystack supports, see [DocumentStore Integrations](https://haystack.deepset.ai/integrations?type=Document+Store)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLVaFHTO-qb"
      },
      "source": [
        "The DocumentStore is now ready. Now it's time to fill it with some Documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HryYZP9ZO-qb"
      },
      "source": [
        "### Fetch the Data\n",
        "\n",
        "You'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents. We preprocessed the data and uploaded to a Hugging Face Space: [Seven Wonders](https://huggingface.co/datasets/bilgeyucel/seven-wonders). Thus, you don't need to perform any additional cleaning or splitting.\n",
        "\n",
        "Fetch the data and convert it into Haystack Documents:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "FJJu86ONfRcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"mvbhat/verdicts\", split=\"train\")\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df = dataset.to_pandas()\n",
        "\n",
        "# Combine the 'facts' and 'verdict' columns into a new 'content' column\n",
        "df['content'] = df['facts'] + \" \" + df['verdict']\n",
        "\n",
        "# Assuming df is your modified DataFrame with the 'content' column\n",
        "new_dataset = dataset.from_pandas(df[['content']])\n",
        "\n",
        "# Converting to Haystack Document objects if needed\n",
        "docs = [Document(content=doc[\"content\"]) for doc in new_dataset]\n"
      ],
      "metadata": {
        "id": "QcNiH59VfIQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "GNlNzQxzfkbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].content"
      ],
      "metadata": {
        "id": "p4Zd_dlffosm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT macadeliccc/US-SupremeCourtVerdicts FROM hugging face\n"
      ],
      "metadata": {
        "id": "tlrAHFU4YJwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "df_SupremeCourt = load_dataset(\"macadeliccc/US-SupremeCourtVerdicts\", split=\"train\")\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df_SupremeCourt = df_SupremeCourt.to_pandas()\n"
      ],
      "metadata": {
        "id": "zq7EPkjYYOeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_SupremeCourt"
      ],
      "metadata": {
        "id": "GQpp91UFYpIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from Hugging Face\n",
        "dataset_supreme_court = load_dataset(\"macadeliccc/US-SupremeCourtVerdicts\", split=\"train\")\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df_supreme_court = dataset_supreme_court.to_pandas()\n",
        "\n",
        "# Combine the 'facts' and 'verdict' columns into a new 'content' column\n",
        "df_supreme_court['content'] = df_supreme_court['category'] + \" \" + df_supreme_court['summary']\n",
        "\n",
        "# Assuming df is your modified DataFrame with the 'content' column\n",
        "new_dataset2 = dataset_supreme_court.from_pandas(df_supreme_court[['content']])\n",
        "\n",
        "# Converting to Haystack Document objects if needed\n",
        "docs2 = [Document(content=doc[\"content\"]) for doc in new_dataset2]"
      ],
      "metadata": {
        "id": "LYNs0fnxaYwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs2[0].content"
      ],
      "metadata": {
        "id": "0h_TnDwsb_H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = docs + docs2"
      ],
      "metadata": {
        "id": "NxUdienucD8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top documents\n"
      ],
      "metadata": {
        "id": "nj8oLGAUcCU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# from collections import Counter, defaultdict\n",
        "\n",
        "# class BM25:\n",
        "#     def __init__(self, documents, k1=1.5, b=0.75):\n",
        "#         self.documents = documents\n",
        "#         self.k1 = k1\n",
        "#         self.b = b\n",
        "#         self.doc_freqs = []\n",
        "#         self.doc_len = []\n",
        "#         self.avgdl = 0\n",
        "#         self.corpus_size = 0\n",
        "#         self.idf = {}\n",
        "\n",
        "#         self._initialize()\n",
        "\n",
        "#     def _initialize(self):\n",
        "#         # Calculate document frequencies for terms\n",
        "#         df = defaultdict(int)\n",
        "#         total_length = 0\n",
        "\n",
        "#         for doc in self.documents:\n",
        "#             self.corpus_size += 1\n",
        "#             total_length += len(doc)\n",
        "#             frequencies = Counter(doc)\n",
        "#             self.doc_freqs.append(frequencies)\n",
        "#             self.doc_len.append(len(doc))\n",
        "#             for word in frequencies:\n",
        "#                 df[word] += 1\n",
        "\n",
        "#         self.avgdl = total_length / self.corpus_size\n",
        "\n",
        "#         # Calculate IDF for each term\n",
        "#         for word, freq in df.items():\n",
        "#             self.idf[word] = math.log(1 + (self.corpus_size - freq + 0.5) / (freq + 0.5))\n",
        "\n",
        "#     def score(self, query, index):\n",
        "#         score = 0.0\n",
        "#         frequencies = self.doc_freqs[index]\n",
        "#         doc_length = self.doc_len[index]\n",
        "\n",
        "#         for term in query:\n",
        "#             if term in frequencies:\n",
        "#                 freq = frequencies[term]\n",
        "#                 idf = self.idf.get(term, 0)\n",
        "#                 term_score = idf * ((freq * (self.k1 + 1)) / (freq + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))))\n",
        "#                 score += term_score\n",
        "\n",
        "#         return score\n",
        "\n",
        "#     def get_scores(self, query):\n",
        "#         scores = []\n",
        "#         for index in range(self.corpus_size):\n",
        "#             scores.append(self.score(query, index))\n",
        "#         return scores\n",
        "\n",
        "#     def retrieve(self, query, top_n=5):\n",
        "#         query = query.split()\n",
        "#         scores = self.get_scores(query)\n",
        "#         ranked_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
        "#         return [(self.documents[i], score) for i, score in ranked_scores[:top_n]]\n",
        "\n",
        "# # # Example usage:\n",
        "# # documents = [\n",
        "# #     \"The Statue of Liberty is located in New York City.\",\n",
        "# #     \"The Colossus of Rhodes was one of the Seven Wonders of the Ancient World.\",\n",
        "# #     \"The Great Wall of China is a historic wall in China.\",\n",
        "# #     \"Machu Picchu is a 15th-century Inca citadel located in Peru.\",\n",
        "# #     \"The Statue of Liberty was a gift from France to the United States.\"\n",
        "# # ]\n",
        "\n",
        "# # bm25 = BM25([doc.split() for doc in documents])\n",
        "\n",
        "# # query = \"Statue of Liberty gift\"\n",
        "# # results = bm25.retrieve(query)\n",
        "\n",
        "# # for doc, score in results:\n",
        "# #     print(f\"Score: {score:.4f}, Document: {' '.join(doc)}\")\n"
      ],
      "metadata": {
        "id": "kISiDRJ1cGE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top documents\n",
        "#results = bm25.retrieve(query)"
      ],
      "metadata": {
        "id": "-ygtQWG7cUj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "2_ajKO4ScWy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INdC3WvLO-qb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from haystack import Document\n",
        "\n",
        "# dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
        "# docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czMjWwnxPA-3"
      },
      "source": [
        "### Initalize a Document Embedder\n",
        "\n",
        "To store your data in the DocumentStore with embeddings, initialize a [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder) with the model name and call `warm_up()` to download the embedding model.\n",
        "\n",
        "> If you'd like, you can use a different [Embedder](https://docs.haystack.deepset.ai/docs/embedders) for your documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUmAH9sEn3R7"
      },
      "outputs": [],
      "source": [
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "doc_embedder.warm_up()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y4iJE_SrS4K"
      },
      "source": [
        "### Write Documents to the DocumentStore\n",
        "\n",
        "Run the `doc_embedder` with the Documents. The embedder will create embeddings for each document and save these embeddings in Document object's `embedding` field. Then, you can write the Documents to the DocumentStore with `write_documents()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "c9a9f57995c3499a8e83e4e8ea6602cf"
          ]
        },
        "id": "ETpQKftLplqh",
        "outputId": "afd8f058-1a5e-4c06-9270-ce8f924ee4c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9a9f57995c3499a8e83e4e8ea6602cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/66 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "docs_with_embeddings = doc_embedder.run(docs)\n",
        "document_store.write_documents(docs_with_embeddings[\"documents\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdojTxg6uubn"
      },
      "source": [
        "## Building the RAG Pipeline\n",
        "\n",
        "The next step is to build a [Pipeline](https://docs.haystack.deepset.ai/docs/pipelines) to generate answers for the user query following the RAG approach. To create the pipeline, you first need to initialize each component, add them to your pipeline, and connect them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uyV6-u-u56P"
      },
      "source": [
        "### Initialize a Text Embedder\n",
        "\n",
        "Initialize a text embedder to create an embedding for the user query. The created embedding will later be used by the Retriever to retrieve relevant documents from the DocumentStore.\n",
        "\n",
        "> âš ï¸ Notice that you used `sentence-transformers/all-MiniLM-L6-v2` model to create embeddings for your documents before. This is why you need to use the same model to embed the user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyJY2yW628dl"
      },
      "outputs": [],
      "source": [
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "# english embedding\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "# Switch to DictaLM-2.0 embedding model from Hugging Face\n",
        "#text_embedder = SentenceTransformersTextEmbedder(model=\"dicta-il/dictalm2.0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# try to dowland hebrew embedding, session crash using all availible ram\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # Example text to embed\n",
        "# sample_text = \"This is a test sentence.\"\n",
        "\n",
        "# # Using the first embedder (all-MiniLM-L6-v2)\n",
        "# model1 = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# embedding1 = model1.encode(sample_text)\n",
        "# print(\"Embedding from all-MiniLM-L6-v2:\", embedding1)\n",
        "\n",
        "# # Using the second embedder (DictaLM-2.0)\n",
        "# model2 = SentenceTransformer(\"dicta-il/dictalm2.0\")\n",
        "# embedding2 = model2.encode(sample_text)\n",
        "# print(\"Embedding from DictaLM-2.0:\", embedding2)\n",
        "\n",
        "# # Optionally, compare the embeddings using cosine similarity\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# similarity = cosine_similarity([embedding1], [embedding2])\n",
        "# print(\"Cosine Similarity between the two embeddings:\", similarity[0][0])\n",
        "\n"
      ],
      "metadata": {
        "id": "2HWbbwdBS0Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_cj-5m-O-qb"
      },
      "source": [
        "### Initialize the Retriever\n",
        "\n",
        "Initialize a [InMemoryEmbeddingRetriever](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever) and make it use the InMemoryDocumentStore you initialized earlier in this tutorial. This Retriever will get the relevant documents to the query."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: ModuleNotFoundError: No module named 'haystack.components.retrievers.sparse'\n",
        "\n",
        "# !pip install farm-haystack[sparse]\n"
      ],
      "metadata": {
        "id": "C9kRSfAPnbtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip uninstall pydantic\n",
        "# ! pip install pydantic==1.10.9\n",
        "# ! pip install --upgrade --force-reinstall haystack-ai\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-60ujJa0XnjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip show haystack-ai"
      ],
      "metadata": {
        "id": "XOl5h2P0RjGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uo-6fjiO-qb"
      },
      "outputs": [],
      "source": [
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "\n",
        "# from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
        "\n",
        "# from haystack.nodes import ElasticsearchRetriever\n",
        "\n",
        "# document_store = ElasticsearchDocumentStore()\n",
        "# es_retriever = ElasticsearchRetriever(document_store=document_store)\n",
        "\n",
        "\n",
        "#bm25_retriever = BM25Retriever(document_store=document_store)\n",
        "\n",
        "# Initialize the BM25 retriever\n",
        "#retriever = BM25Retriever(document_store=document_store)\n",
        "\n",
        "# from haystack.nodes import TfidfRetriever\n",
        "# tfidf_retriever = TfidfRetriever(document_store=document_store)\n",
        "\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_LnsVVBSKCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CEuQpB7O-qb"
      },
      "source": [
        "### Define a Template Prompt\n",
        "\n",
        "Create a custom prompt for a generative question answering task using the RAG approach. The prompt should take in two parameters: `documents`, which are retrieved from a document store, and a `question` from the user. Use the Jinja2 looping syntax to combine the content of the retrieved documents in the prompt.\n",
        "\n",
        "Next, initialize a [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder) instance with your prompt template. The PromptBuilder, when given the necessary values, will automatically fill in the variable values and generate a complete prompt. This approach allows for a more tailored and effective question-answering experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObahTh45FqOT"
      },
      "outputs": [],
      "source": [
        "# from haystack.components.builders import PromptBuilder\n",
        "\n",
        "# template = \"\"\"\n",
        "# Given the following information, answer the question.\n",
        "\n",
        "# Context:\n",
        "# {% for document in documents %}\n",
        "#     {{ document.content }}\n",
        "# {% endfor %}\n",
        "\n",
        "# Question: {{question}}\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "\n",
        "# prompt_builder = PromptBuilder(template=template)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "template = \"\"\"\n",
        "You are a legal expert. Given the following legal context, provide a detailed and accurate response to the question.\n",
        "\n",
        "Legal Context:\n",
        "{% for document in documents %}\n",
        "    - Case Summary: {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{question}}\n",
        "\n",
        "As an expert in law, consider the following:\n",
        "- The minimal and maximal range of punishment imposed for the described offenses.\n",
        "- Any differences in punishment based on the presence or absence of the defendant's remorse.\n",
        "- The frequency with which certain circumstances occur together in similar cases.\n",
        "\n",
        "Provide a well-reasoned answer based on the context provided:\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=template)"
      ],
      "metadata": {
        "id": "JK69b7CDYnpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR14lbfcFtXj"
      },
      "source": [
        "### Initialize a Generator\n",
        "\n",
        "\n",
        "Generators are the components that interact with large language models (LLMs). Now, set `OPENAI_API_KEY` environment variable and initialize a [OpenAIGenerator](https://docs.haystack.deepset.ai/docs/OpenAIGenerator) that can communicate with OpenAI GPT models. As you initialize, provide a model name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SavE_FAqfApo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
        "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nenbo2SvycHd"
      },
      "source": [
        "> You can replace `OpenAIGenerator` in your pipeline with another `Generator`. Check out the full list of generators [here](https://docs.haystack.deepset.ai/docs/generators)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bfHwOQwycHe"
      },
      "source": [
        "### Build the Pipeline\n",
        "\n",
        "To build a pipeline, add all components to your pipeline and connect them. Create connections from `text_embedder`'s \"embedding\" output to \"query_embedding\" input of `retriever`, from `retriever` to `prompt_builder` and from `prompt_builder` to `llm`. Explicitly connect the output of `retriever` with \"documents\" input of the `prompt_builder` to make the connection obvious as `prompt_builder` has two inputs (\"documents\" and \"question\").\n",
        "\n",
        "For more information on pipelines and creating connections, refer to [Creating Pipelines](https://docs.haystack.deepset.ai/docs/creating-pipelines) documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6NFmpjEO-qb"
      },
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "basic_rag_pipeline = Pipeline()\n",
        "# Add components to your pipeline\n",
        "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
        "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "basic_rag_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "# Now, connect the components to each other\n",
        "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NqyLhx7O-qc"
      },
      "source": [
        "That's it! Your RAG pipeline is ready to generate answers to questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBAyF5tVO-qc"
      },
      "source": [
        "## Asking a Question\n",
        "\n",
        "When asking a question, use the `run()` method of the pipeline. Make sure to provide the question to both the `text_embedder` and the `prompt_builder`. This ensures that the `{{question}}` variable in the template prompt gets replaced with your specific question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnt283M5O-qc"
      },
      "outputs": [],
      "source": [
        "question = \"What was the court's decision in the case involving the Idaho Probate Code and its preference for men over women in estate administration?\"\n",
        "\n",
        "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \": What was the Court's decision regarding the protection of obscene materials under the First Amendment, and how did it modify the test for obscenity?\"\n",
        "\n",
        "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
        "\n",
        "print(response[\"llm\"][\"replies\"][0])"
      ],
      "metadata": {
        "id": "B0b1ZLgQRtUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  No module named 'deepeval'\n",
        "\n",
        "!pip install deepeval\n"
      ],
      "metadata": {
        "id": "AFZk5VLif2Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import (\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "\n",
        "contextual_precision = ContextualPrecisionMetric()\n",
        "contextual_recall = ContextualRecallMetric()\n",
        "contextual_relevancy = ContextualRelevancyMetric()\n",
        "\n",
        "# Load your CSV file\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"mvbhat/verdicts\", split=\"train\")\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df = dataset.to_pandas()\n",
        "\n",
        "# Example questions with expected outputs and relevant contexts from the CSV\n",
        "test_cases_data = [\n",
        "    {\n",
        "        \"question\": \"What is the maximum punishment given in cases involving mass mailing campaigns?\",\n",
        "        \"expected_output\": \"The punishment for cases involving mass mailing campaigns typically involves restrictions or penalties related to obscenity laws, with potential fines or imprisonment depending on the severity of the offense.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"mass mailing campaign\")]['facts'].iloc[0]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the range of punishment in cases involving forgery?\",\n",
        "        \"expected_output\": \"The punishment for forgery typically includes fines and imprisonment, depending on the severity and circumstances of the offense.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"forged money orders\")]['facts'].iloc[0]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How often are gender-based inheritance rules upheld in court?\",\n",
        "        \"expected_output\": \"Gender-based inheritance rules are rarely upheld in modern courts, particularly when they conflict with equal protection under the law.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"Idaho Probate Code\")]['facts'].iloc[0]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What was the outcome of cases involving obscenity laws?\",\n",
        "        \"expected_output\": \"Cases involving obscenity laws often result in convictions or penalties, with the Supreme Court affirming that obscene materials are not protected under the First Amendment.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"obscene material\")]['facts'].iloc[0]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How are cases involving reproductive rights typically resolved?\",\n",
        "        \"expected_output\": \"Cases involving reproductive rights often focus on privacy and personal liberty, with the courts generally protecting these rights.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"Jane Roe\")]['facts'].iloc[0]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the range of punishments for cases involving free speech?\",\n",
        "        \"expected_output\": \"Punishments in cases involving free speech can vary widely but often involve fines or imprisonment when speech is determined to incite violence or harm.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"obscene material\")]['facts'].iloc[0]\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"question\": \"How do courts rule in cases involving privacy rights?\",\n",
        "        \"expected_output\": \"Courts often uphold privacy rights, particularly in matters involving personal autonomy and medical decisions.\",\n",
        "        \"relevant_context\": df[df['facts'].str.contains(\"Jane Roe\")]['facts'].iloc[0]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Initialize a dictionary to store the results\n",
        "results = {}\n",
        "\n",
        "\n",
        "\n",
        "# Loop through each test case\n",
        "for i, case in enumerate(test_cases_data):\n",
        "    response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": case[\"question\"]}, \"prompt_builder\": {\"question\": case[\"question\"]}})\n",
        "    test_case = LLMTestCase(\n",
        "        input=case[\"question\"],\n",
        "        actual_output=response[\"llm\"][\"replies\"][0],\n",
        "        expected_output=case[\"expected_output\"],\n",
        "        retrieval_context=[case[\"relevant_context\"]] if case[\"relevant_context\"] else []\n",
        "    )\n",
        "\n",
        "    # Evaluate the test case\n",
        "    contextual_precision.measure(test_case)\n",
        "    contextual_recall.measure(test_case)\n",
        "    contextual_relevancy.measure(test_case)\n",
        "\n",
        "    # Store the results in the dictionary\n",
        "    results[f\"Test_{i+1}\"] = {\n",
        "        \"precision_score\": contextual_precision.score,\n",
        "        \"precision_reason\": contextual_precision.reason,\n",
        "        \"recall_score\": contextual_recall.score,\n",
        "        \"recall_reason\": contextual_recall.reason,\n",
        "        \"relevancy_score\": contextual_relevancy.score,\n",
        "        \"relevancy_reason\": contextual_relevancy.reason\n",
        "    }\n",
        "\n",
        "# The 'results' dictionary now contains the evaluation results for all test cases\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "Vna1S8rqfxiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "dpVprb0WoTFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result data from the test cases\n",
        "result = {\n",
        "    'Test_1': {'precision_score': 1.0, 'recall_score': 0.5, 'relevancy_score': 0.0},\n",
        "    'Test_2': {'precision_score': 0, 'recall_score': 0.0, 'relevancy_score': 0.0},\n",
        "    'Test_3': {'precision_score': 1.0, 'recall_score': 0.0, 'relevancy_score': 0.0},\n",
        "    'Test_4': {'precision_score': 1.0, 'recall_score': 0.5, 'relevancy_score': 0.0},\n",
        "    'Test_5': {'precision_score': 0, 'recall_score': 0.0, 'relevancy_score': 0},\n",
        "    'Test_6': {'precision_score': 1.0, 'recall_score': 0.0, 'relevancy_score': 0.0},\n",
        "    'Test_7': {'precision_score': 0, 'recall_score': 0.0, 'relevancy_score': 0.0},\n",
        "    'Test_8': {'precision_score': 0, 'recall_score': 0.0, 'relevancy_score': 0},\n",
        "    'Test_9': {'precision_score': 1.0, 'recall_score': 0.0, 'relevancy_score': 0.0},\n",
        "    'Test_10': {'precision_score': 0, 'recall_score': 0.0, 'relevancy_score': 0.0}\n",
        "}\n",
        "\n",
        "# Calculate the average scores\n",
        "total_precision = sum(test['precision_score'] for test in result.values())\n",
        "total_recall = sum(test['recall_score'] for test in result.values())\n",
        "total_relevancy = sum(test['relevancy_score'] for test in result.values())\n",
        "\n",
        "num_tests = len(result)\n",
        "\n",
        "avg_precision = total_precision / num_tests\n",
        "avg_recall = total_recall / num_tests\n",
        "avg_relevancy = total_relevancy / num_tests\n",
        "\n",
        "avg_precision, avg_recall, avg_relevancy"
      ],
      "metadata": {
        "id": "R_4f93uGo5mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**basic prompt (0.5, 0.1, 0.0)**\n",
        "\n",
        "Precision (0.5):  RAG pipeline is retrieving some relevant information, but not consistently across all cases.\n",
        "\n",
        "Recall (0.1):  RAG pipeline is missing a significant amount of relevant information that should be retrieved to answer the queries comprehensively.\n",
        "\n",
        "Relevancy (0.0): The information being retrieved is not contextually appropriate, meaning it does not directly answer or relate well to the queries.\n",
        "\n",
        "after quesry modification (0.5, 0.1, 0.0)"
      ],
      "metadata": {
        "id": "qlNTCNzlpL9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**after query modification (0.5, 0.1, 0.0)**"
      ],
      "metadata": {
        "id": "Tu6UJZ73diOP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQN-aoGO-qc"
      },
      "source": [
        "Here are some other example questions to test:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " df[df['facts'].str.contains(\"mass mailing campaign\")]['facts'].iloc[0]"
      ],
      "metadata": {
        "id": "7WJPxRKrG26f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XueCK3y4O-qc"
      },
      "source": [
        "## What's next\n",
        "\n",
        "ðŸŽ‰ Congratulations! You've learned how to create a generative QA system for your documents with the RAG approach.\n",
        "\n",
        "If you liked this tutorial, you may also enjoy:\n",
        "- [Filtering Documents with Metadata](https://haystack.deepset.ai/tutorials/31_metadata_filtering)\n",
        "- [Preprocessing Different File Types](https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline)\n",
        "- [Creating a Hybrid Retrieval Pipeline](https://haystack.deepset.ai/tutorials/33_hybrid_retrieval)\n",
        "\n",
        "To stay up to date on the latest Haystack developments, you can [subscribe to our newsletter](https://landing.deepset.ai/haystack-community-updates) and [join Haystack discord community](https://discord.gg/haystack).\n",
        "\n",
        "Thanks for reading!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}